{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Tuple, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, Xception\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "DFDC_PATH = \"../../../DeepFake_Detection/DFDC_ALL_DATA_224/\"\n",
    "DFDC_METADATA = \"../../../DeepFake_Detection/DFDC_ALL_DATA/metadata/metadata.json\"\n",
    "WD_TRAIN_CSV_PATH = \"../datasets/train/min/train_wasserstein_distance-min_length-3_dataset.csv\"\n",
    "DATA_PATH = \"../../../DeepFake_Detection/WILDDEEP_DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_path_generator(csv_path: str, data_path: str) -> Tuple[List[List[str]], List[str]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    X_paths, y = [], []\n",
    "    for i in range(len(df.index)):\n",
    "        subset = df.iloc[i].subset\n",
    "        video_type = df.iloc[i].type\n",
    "        video_number = df.iloc[i].video\n",
    "        sequence_number = df.iloc[i].sequence\n",
    "        first_frame = df.iloc[i].first_frame\n",
    "        subsequence_length = df.iloc[i].subsequence_length\n",
    "        \n",
    "        subsequence = []\n",
    "        for j in range(first_frame, first_frame + subsequence_length):\n",
    "            path =  data_path + f\"{video_type}_{subset}/\" + str(video_number) + \"/\" + video_type + \"/\" + str(sequence_number) + \"/\" + str(j) + \".png\"\n",
    "            subsequence.append(path)\n",
    "        \n",
    "        X_paths.append(subsequence)\n",
    "\n",
    "        y.append(0 if video_type == \"real\" else 1)\n",
    "\n",
    "    return X_paths, y\n",
    "\n",
    "def ravel_and_match_lists(X: List[List], y: List) -> Tuple[List, List]:\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            x_out.append(X[i][j])\n",
    "            y_out.append(y[i])\n",
    "    \n",
    "\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfdc_path(data_path: str, file: str) -> str:\n",
    "    path = data_path + file.replace(\".mp4\", \".jpg\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: At some point change to handle sequences of frames \n",
    "def load_dfdc_paths(metadata_path: str, data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    X_paths, y = [], []\n",
    "    labels = [\"REAL\", \"FAKE\"]\n",
    "\n",
    "    dfdc_paths = pd.read_json(DFDC_METADATA)\n",
    "    dfdc_files = list(dfdc_paths.columns.values)\n",
    "\n",
    "    for file in tqdm(dfdc_files):\n",
    "        try:\n",
    "            X_paths.append(get_dfdc_path(data_path, file))\n",
    "            y.append(labels.index(dfdc_paths[file]['label']))\n",
    "        except Exception as err:\n",
    "            #print(err)\n",
    "            pass\n",
    "    \n",
    "    return X_paths, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lists(a: List, b: List, seed: int=0) -> Tuple[List, List]:\n",
    "    lists = list(zip(a, b))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lists)\n",
    "\n",
    "    a, b = zip(*lists)\n",
    "    a = list(a)\n",
    "    b = list(b)\n",
    "\n",
    "    return (a, b)\n",
    "\n",
    "def shuffle_arrays(a: np.ndarray, b: np.ndarray, seed: int=0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    assert len(a) == len(b)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    return a[permutation], b[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Add multi-feature resampling\n",
    "def resample_dataset(X: np.ndarray, y: np.ndarray, resampling_type: str, final_ratio: float, seed: int) -> Tuple[List, List]:\n",
    "    if resampling_type == \"undersample\":\n",
    "        sampler = RandomUnderSampler(sampling_strategy = final_ratio, random_state = seed)\n",
    "    elif resampling_type == \"oversample\":\n",
    "        sampler = RandomOverSampler(sampling_strategy = final_ratio, random_state = seed)\n",
    "    else:\n",
    "        raise Exception(\"Unknown resampling type. Available types: 'undersample', 'oversample'.\")\n",
    "    \n",
    "    X, y = sampler.fit_resample(X, y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119154/119154 [00:12<00:00, 9835.40it/s] \n"
     ]
    }
   ],
   "source": [
    "X_dfdc, y_dfdc = load_dfdc_paths(DFDC_METADATA, DFDC_PATH)\n",
    "X_dfdc, y_dfdc = np.array(X_dfdc), np.array(y_dfdc)\n",
    "X_dfdc, y_dfdc = X_dfdc.reshape(-1,1), y_dfdc.reshape(-1,1)\n",
    "\n",
    "X_wd_train, y_wd_train = custom_path_generator(WD_TRAIN_CSV_PATH, DATA_PATH)\n",
    "X_wd_train, y_wd_train = ravel_and_match_lists(X_wd_train, y_wd_train)\n",
    "X_wd_train, y_wd_train = shuffle_lists(X_wd_train, y_wd_train)\n",
    "X_wd_train, y_wd_train = np.array(X_wd_train), np.array(y_wd_train)\n",
    "X_wd_train, y_wd_train = X_wd_train.reshape(-1,1), y_wd_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114344, 1), (114344, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dfdc.shape, y_dfdc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37116, 1), (37116,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dfdc, y_dfdc = resample_dataset(X_dfdc, y_dfdc, \"undersample\", 1, 1)\n",
    "X_dfdc.shape, y_dfdc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19518, 1), (19518, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wd_train.shape, y_wd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25981, 1), (11135, 1), (11135, 1), (11135,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dfdc_train, X_dfdc_test, y_dfdc_train, y_dfdc_test = train_test_split(X_dfdc, y_dfdc, test_size=0.3)\n",
    "X_dfdc_train.shape, X_dfdc_test.shape, X_dfdc_test.shape, y_dfdc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack([X_dfdc_train, X_wd_train])\n",
    "y_train = np.vstack([y_dfdc_train.reshape(-1,1), y_wd_train])\n",
    "\n",
    "X_train, y_train = shuffle_arrays(X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45499, 1), (45499, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['../../../DeepFake_Detection/WILDDEEP_DATA/real_train/461/real/238/3232.png'],\n",
       "       ['../../../DeepFake_Detection/WILDDEEP_DATA/real_train/511/real/603/1604.png'],\n",
       "       ['../../../DeepFake_Detection/DFDC_ALL_DATA_224/hnambobzhb.jpg'],\n",
       "       ['../../../DeepFake_Detection/DFDC_ALL_DATA_224/ggzcegccry.jpg'],\n",
       "       ['../../../DeepFake_Detection/DFDC_ALL_DATA_224/ejevdfyyzw.jpg'],\n",
       "       ['../../../DeepFake_Detection/DFDC_ALL_DATA_224/oojvczbnse.jpg'],\n",
       "       ['../../../DeepFake_Detection/WILDDEEP_DATA/real_train/189/real/92/1235.png'],\n",
       "       ['../../../DeepFake_Detection/WILDDEEP_DATA/fake_train/498/fake/32/1152.png'],\n",
       "       ['../../../DeepFake_Detection/WILDDEEP_DATA/real_train/336/real/23/454.png'],\n",
       "       ['../../../DeepFake_Detection/DFDC_ALL_DATA_224/ldnbyjrhwx.jpg']],\n",
       "      dtype='<U75')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path: str) -> cv.Mat:\n",
    "    return cv.cvtColor(cv.imread(path),cv.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: np.ndarray) -> np.ndarray:\n",
    "    output = []\n",
    "\n",
    "    for data in tqdm(dataset):\n",
    "        batch = []\n",
    "        for path in data:\n",
    "            batch.append(read_img(path))\n",
    "        output.append(batch)\n",
    "\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45499/45499 [01:01<00:00, 741.56it/s]\n",
      "100%|██████████| 11135/11135 [00:10<00:00, 1014.43it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = load_dataset(X_train)\n",
    "X_test = load_dataset(X_dfdc_test)\n",
    "y_test = y_dfdc_test.reshape(-1,1)\n",
    "\n",
    "del X_dfdc_test\n",
    "del y_dfdc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45499, 1, 224, 224, 3), (45499, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], \n",
    "                           X_train.shape[2], \n",
    "                           X_train.shape[3], \n",
    "                           X_train.shape[4]\n",
    "))\n",
    "X_test = X_test.reshape((X_test.shape[0], \n",
    "                         X_test.shape[2], \n",
    "                         X_test.shape[3], \n",
    "                         X_test.shape[4]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "#     (X_train, y_train)).shuffle(50000).batch(4)\n",
    "\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(include_top=False, weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model: Model) -> Model:\n",
    "    input_layer = Input(shape = X_train.shape[1::], name = \"input_layer\")\n",
    "    x = model(input_layer)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=\"relu\", name=\"fc1\")(x)\n",
    "    x = Dense(1024, activation=\"relu\", name=\"fc2\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "    result_model = Model(inputs = input_layer, outputs = x)\n",
    "\n",
    "    return result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, None, None, 2048)  20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 24,010,281\n",
      "Trainable params: 23,955,753\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(images, labels):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(images, training=True)\n",
    "#         loss = tf.keras.losses.binary_crossentropy(labels, predictions, from_logits=True)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "#     train_loss(loss)\n",
    "#     train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def test_step(images, labels):\n",
    "#   predictions = model(images, training=False)\n",
    "#   t_loss = tf.keras.losses.binary_crossentropy(labels, predictions, from_logits=True)\n",
    "\n",
    "#   test_loss(t_loss)\n",
    "#   test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! Currently commented out graph execution because of memory limit\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   # Reset the metrics at the start of the next epoch\n",
    "#   train_loss.reset_states()\n",
    "#   train_accuracy.reset_states()\n",
    "#   test_loss.reset_states()\n",
    "#   test_accuracy.reset_states()\n",
    "\n",
    "#   for images, labels in train_ds:\n",
    "#     train_step(images, labels)\n",
    "\n",
    "#   for test_images, test_labels in test_ds:\n",
    "#     test_step(test_images, test_labels)\n",
    "\n",
    "#   print(\n",
    "#     f'Epoch {epoch + 1}, '\n",
    "#     f'Loss: {train_loss.result()}, '\n",
    "#     f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "#     f'Test Loss: {test_loss.result()}, '\n",
    "#     f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "#   )\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, batch_size = 32, epochs=5, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2e24267c165a1ce10e3342bdb7ba0ae1467706fe54c2e010f21b47024b1eb0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
